{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a3c2252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import h5py\n",
    "from astropy.io import fits\n",
    "from astropy.convolution import Gaussian1DKernel, convolve\n",
    "from scipy import interpolate\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d7f319",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = 'ATCA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5101f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory path where your files are located\n",
    "directory_path = '../ATCA_HI_spectra/Hydra'\n",
    "\n",
    "# Common part of the filename\n",
    "common_part_v4 = '_cube_4k_atca_4k_spectrum.txt'\n",
    "# Common part of the filename\n",
    "common_part_v2 = '_cube_v2_atca_spectrum.txt'\n",
    "\n",
    "# List all files in the directory\n",
    "all_files = os.listdir(directory_path)\n",
    "\n",
    "# Filter files based on the common part of the filename\n",
    "files_spectra_v4 = [file for file in all_files if common_part_v4 in file]\n",
    "\n",
    "# Filter files based on the common part of the filename\n",
    "files_spectra_v2 = [file for file in all_files if common_part_v2 in file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e2881-7401-4479-a3ca-7450bc329492",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00a13b9-5dd6-4cf2-83eb-3be6829084ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = np.loadtxt('ATCA_HI_spectra/Hydra/j103913-2509_cube_4k_atca_4k_spectrum.txt', skiprows=1)\n",
    "# Select one random file name from each list if they are not empty\n",
    "random_file_v4 = random.choice(files_spectra_v4) if files_spectra_v4 else None\n",
    "random_file_v2 = random.choice(files_spectra_v2) if files_spectra_v2 else None    \n",
    "\n",
    "data = np.loadtxt(f'../ATCA_HI_spectra/Hydra/{random_file_v4}', skiprows = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d4e1f-26fa-44cb-aa4d-0160040b6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity = data[:, 1]\n",
    "amplitude = data[:, 2]\n",
    "\n",
    "#Measue variability\n",
    "rms_spect = np.sqrt(np.mean(amplitude**2))\n",
    "\n",
    "#Mask signal channels\n",
    "spectrum = np.copy(amplitude)\n",
    "spectrum[spectrum < rms_spect - (np.max(spectrum)-rms_spect) ] = rms_spect\n",
    "\n",
    "#Normalization\n",
    "y = spectrum\n",
    "x = velocity\n",
    "pars = np.polyfit(x, y, 1)\n",
    "p = np.poly1d(pars)\n",
    "tauhi = np.array(amplitude/p(x))\n",
    "\n",
    "# Tau\n",
    "tau = np.log(tauhi)*(-1)\n",
    "rms = np.std(tau)\n",
    "#Smoothing\n",
    "g = Gaussian1DKernel(1)\n",
    "tau_smooth = convolve(tau, g, boundary='extend')\n",
    "rms_smooth = np.std(tau_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c3152-a551-4de9-a7c5-765a45f4f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), gridspec_kw={'height_ratios': [2, 1]})\n",
    "ax1.plot(x, tau, linestyle=':', linewidth=1, color='b', label='Signal')\n",
    "ax1.plot(x, tau_smooth, linestyle='-', linewidth=1, color='r', label='Smoothed')\n",
    "\n",
    "\n",
    "#ax1.axhspan(-rms, + rms*3, alpha=0.2, color='orange')\n",
    "ax1.axhspan(-rms_smooth, + rms_smooth, alpha=0.2, color='g')\n",
    "ax1.axhline(0, color='k', linestyle='--', linewidth=2)\n",
    "ax1.axhline( rms_smooth*2.5, color='r', linestyle='--', linewidth=1)\n",
    "ax1.axhline( np.std(tau)*2.5, color='b', linestyle='--', linewidth=1)\n",
    "#ax1.set_xlabel('v[Km/s]', fontsize=15)\n",
    "ax1.set_ylabel(r'${\\tau}$', fontsize=15)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Add legend\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(x, tauhi, color = 'g')\n",
    "ax2.axhspan(1-np.std(tauhi), 1+ np.std(tauhi), alpha=0.2, color='grey')\n",
    "ax2.axhline(1, color='k', linestyle='--', linewidth=2)\n",
    "# Label the points where tauhi < 1 - 3 * std_tauhi with dots\n",
    "median_tauhi = np.median(tauhi)\n",
    "mad_tauhi = np.median(np.abs(tauhi - median_tauhi))\n",
    "below_threshold = tauhi < (1 - 3.5 * mad_tauhi)\n",
    "ax2.plot(x[below_threshold], tauhi[below_threshold], 'ro')  # Red dots\n",
    "ax2.set_xlabel('v[Km/s]', fontsize=15)\n",
    "ax2.set_ylabel(r'$e^{-\\tau}$')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d4a2e-7b7a-4365-a9ee-8dac2b1fa44a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31809d-5f50-4b16-b75f-6fd77a225654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.convolution import convolve, Gaussian1DKernel\n",
    "\n",
    "def plot_spectra(files_spectra, gkernel, image_name=None, source=None):\n",
    "    # Calculate the number of rows and columns for subplots\n",
    "    num_files = len(files_spectra)\n",
    "    num_rows = int(np.ceil(num_files / 3))  # 3 subplots per row\n",
    "\n",
    "    # Create a single figure with multiple subplots\n",
    "    fig, axes = plt.subplots(num_rows, 3, figsize=(18, 8*num_rows))\n",
    "    axes = axes.flatten()  # Flatten the 2D array of axes for easier iteration\n",
    "\n",
    "    # Iterate through matching files and create subplots for each file\n",
    "    for i, file_name in enumerate(files_spectra):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "        # Read data from the text file into a NumPy array, skipping the header row\n",
    "        data = np.loadtxt(file_path, skiprows=1)\n",
    "\n",
    "        # Extract velocity and amplitude columns\n",
    "        velocity = data[:, 1]\n",
    "        amplitude = data[:, 2]\n",
    "        \n",
    "        # Measure variability\n",
    "        rms_spect = np.sqrt(np.mean(amplitude**2))\n",
    "        \n",
    "        # Mask signal channels\n",
    "        spectrum = np.copy(amplitude)\n",
    "        spectrum[spectrum < rms_spect - (np.max(spectrum) - rms_spect)] = rms_spect\n",
    "        \n",
    "        # Normalization\n",
    "        y = spectrum\n",
    "        x = velocity\n",
    "        pars = np.polyfit(x, y, 1)\n",
    "        p = np.poly1d(pars)\n",
    "        tauhi = np.array(amplitude / p(x))\n",
    "        \n",
    "        # Tau\n",
    "        tau = np.log(tauhi) * (-1)\n",
    "        rms = np.std(tau)\n",
    "        \n",
    "        # Smoothing\n",
    "        g = Gaussian1DKernel(gkernel)\n",
    "        tau_smooth = convolve(tau, g, boundary='extend')\n",
    "        rms_smooth = np.std(tau_smooth)\n",
    "\n",
    "        # Create a nested grid within each subplot\n",
    "        outer_ax = axes[i]\n",
    "        gs = outer_ax.get_gridspec()\n",
    "        outer_ax.remove()\n",
    "\n",
    "        inner_gs = gs[i].subgridspec(2, 1, height_ratios=[3, 1])\n",
    "        ax1 = fig.add_subplot(inner_gs[0])\n",
    "        ax2 = fig.add_subplot(inner_gs[1])\n",
    "\n",
    "        # Plotting the first panel\n",
    "        ax1.plot(x, tau, linestyle=':', linewidth=1, color='b', alpha = 0.4, label='Signal')\n",
    "        ax1.plot(x, tau_smooth, linestyle='-', linewidth=1, color='r', label='Smoothed')\n",
    "        ax1.axhspan(-rms_smooth, rms_smooth, alpha=0.2, color='g')\n",
    "        ax1.axhline(0, color='k', linestyle='--', linewidth=2)\n",
    "        ax1.axhline(rms_smooth * 3, color='k', linestyle='--', linewidth=1)\n",
    "        ax1.set_ylabel(r'${\\tau}$', fontsize=15)\n",
    "        ax1.grid(True)\n",
    "        ax1.legend()\n",
    "        ax1.set_title(f'File: {file_name}')\n",
    "\n",
    "        # Plotting the second panel\n",
    "        ax2.plot(x, tauhi, color='g')\n",
    "        ax2.axhspan(1 - np.std(tauhi), 1 + np.std(tauhi), alpha=0.2, color='grey')\n",
    "        ax2.axhline(1, color='k', linestyle='--', linewidth=2)\n",
    "        ax2.set_xlabel('v[Km/s]', fontsize=15)\n",
    "        ax2.set_ylabel(r'$e^{-\\tau}$')\n",
    "\n",
    "        # Title for the overall plot area\n",
    "        #outer_ax.set_title(f'File: {file_name}')\n",
    "\n",
    "    # Hide any remaining empty subplots, if any\n",
    "    for j in range(num_files, num_rows * 3):\n",
    "        fig.delaxes(axes[j])\n",
    "\n",
    "    # Adjust layout and display the figure\n",
    "    plt.tight_layout()\n",
    "    if image_name is not None:\n",
    "        # Ensure the output directory exists\n",
    "        output_dir = os.path.join('../Images', source)  # Change Norma or Hydra\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # Save the figure\n",
    "        output_path = os.path.join(output_dir, f'{image_name}.pdf')\n",
    "        plt.savefig(output_path, format='pdf')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44e1076",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_spectra(files_spectra_v4, 1)\n",
    "#plot_spectra(files_spectra_v4,1,'spectra_4k_hydraG1',source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51376b3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_spectra(files_spectra_v2,4)\n",
    "plot_spectra(files_spectra_v2,4, 'spectra_v2_hydraG4',source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06561f3-c9fa-4bdb-b4c9-e94397f65b5f",
   "metadata": {},
   "source": [
    "## Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a06786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nans(array_cube):\n",
    "    nan_mask_3d = np.isnan(array_cube)\n",
    "\n",
    "    # Get the indices of NaN values\n",
    "    nan_indices_3d = np.where(nan_mask_3d)\n",
    "\n",
    "    print(nan_indices_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7cf23f",
   "metadata": {},
   "source": [
    "### 4 km/s resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1d594c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gkernel = 1.\n",
    "crval3_4k_list = []\n",
    "spect_4k_list = []\n",
    "files_name_4k = []\n",
    "\n",
    "spect_4k = np.zeros((140, 5, 6))     #Resolution of 4 km/s \n",
    "files_4k = {}\n",
    "# spect_4k_2 = []\n",
    "# spect_4k_1 = []\n",
    "# files_name_4k1 = []\n",
    "# files_name_4k2 = []\n",
    "for i, file_name in enumerate(files_spectra_v4):\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    \n",
    "    # Read data from the text file into a NumPy array, skipping the header row\n",
    "    data = np.loadtxt(file_path, skiprows=1)\n",
    "\n",
    "    # Extract velocity and amplitude columns\n",
    "    velocity = data[:, 1]\n",
    "    amplitude = data[:, 2]\n",
    "    \n",
    "    #Measue variability\n",
    "    rms_spect = np.sqrt(np.mean(amplitude**2))\n",
    "    \n",
    "    #Mask signal channels\n",
    "    spectrum = np.copy(amplitude)\n",
    "    spectrum[spectrum < rms_spect - (np.max(spectrum)-rms_spect) ] = rms_spect\n",
    "    \n",
    "    #Normalization\n",
    "    y = spectrum\n",
    "    x = velocity\n",
    "    pars = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(pars)\n",
    "    tauhi = np.array(amplitude/p(x))\n",
    "    \n",
    "    # Tau\n",
    "    tau = np.log(tauhi)*(-1)\n",
    "    rms = np.std(tau)\n",
    "    #Smoothing\n",
    "    g = Gaussian1DKernel(gkernel)\n",
    "    tau_smooth = convolve(tau, g, boundary='extend')\n",
    "    crval3_4k_list.append(velocity[0])\n",
    "    spect_4k_list.append(tau_smooth)\n",
    "    files_name_4k.append(file_name.split('_cube')[0])\n",
    "    # if velocity[0] > -193:\n",
    "    #     spect_4k_1.append(tau_smooth)\n",
    "    #     files_name_4k1.append(file_name.split('_cube')[0])\n",
    "    # else:\n",
    "    #     spect_4k_2.append(tau_smooth)\n",
    "    #     files_name_4k2.append(file_name.split('_cube')[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cf866d-d9ec-45fa-aa1c-39af6edd6d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(len(spect_4k_list))\n",
    "#print(len(spect_4k_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e648862a-e4f0-4368-ae41-3e697d2d63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "spect_4k = np.zeros((140, 5, 6))     #Resolution of 4 km/s \n",
    "files_4k = {}\n",
    "# spect_4k1 = np.zeros((140, 3, 9))     #Resolution of 4 km/s \n",
    "# spect_4k2 = np.zeros((140, 1, 3))     #Resolution of 4 km/s \n",
    "# files_4k1 = {}\n",
    "# files_4k2 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d557395f-265f-4dfc-adac-66313bfb2d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crval3_4k_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2725472-67ab-41cf-99fa-3f3b5469c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, data in enumerate(spect_4k_list):\n",
    "    index1 = i // 6  # Row index\n",
    "    index2 = i % 6  # Column index\n",
    "    spect_4k[:, index1, index2] = data\n",
    "    \n",
    "    # Store source's name and position in the dict\n",
    "    files_4k[files_name_4k[i]] = {\"pos\":(index2, index1), \"vel\": crval3_4k_list[i]}\n",
    "\n",
    "# for i, data in enumerate(spect_4k_1):\n",
    "#     index1 = i // 9  # Row index\n",
    "#     index2 = i % 9  # Column index\n",
    "#     spect_4k1[:, index1, index2] = data\n",
    "    \n",
    "#     # Store source's name and position in the dict\n",
    "#     files_4k1[files_name_4k1[i]] = {\"pos\":(index2, index1), \"index\": i}\n",
    "\n",
    "# for i, data in enumerate(spect_4k_2):\n",
    "#     index1 = i // 3  # Row index\n",
    "#     index2 = i % 3   # Column index\n",
    "#     spect_4k2[:, index1, index2] = data\n",
    "\n",
    "#     # Store source's name and position in the dict\n",
    "#     files_4k2[files_name_4k2[i]] = {\"pos\":(index2, index1), \"index\": i}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e756f0b2-a386-45e1-a347-b9b968a9e318",
   "metadata": {},
   "source": [
    "### 0.2 km/s resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bd9bd64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9677/1140235763.py:46: RuntimeWarning: invalid value encountered in log\n",
      "  tau = np.log(tauhi)*(-1)\n"
     ]
    }
   ],
   "source": [
    "spect_v2_1000 = []\n",
    "spect_v2_1500 = []\n",
    "\n",
    "files_name_v2_1000 = []\n",
    "files_name_v2_1500 = []\n",
    "# spect_v2_10001 = []\n",
    "# spect_v2_10002 = []\n",
    "# spect_v2_1500 = []\n",
    "\n",
    "# files_name_v2_10001 = []\n",
    "# files_name_v2_10002 = []\n",
    "# files_name_v2_1500 = []\n",
    "\n",
    "gkernel = 1\n",
    "\n",
    "crval3_v2_1000list = []\n",
    "crval3_v2_1500list = []\n",
    "\n",
    "for i, file_name in enumerate(files_spectra_v2):\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_path = os.path.join(directory_path, file_name)\n",
    "    \n",
    "    # Read data from the text file into a NumPy array, skipping the header row\n",
    "    data = np.loadtxt(file_path, skiprows=1)\n",
    "    \n",
    "    # Extract velocity and amplitude columns\n",
    "    velocity = data[:, 1]\n",
    "    amplitude = data[:, 2]\n",
    "    \n",
    "    #Measue variability\n",
    "    rms_spect = np.sqrt(np.mean(amplitude**2))\n",
    "    \n",
    "    #Mask signal channels\n",
    "    spectrum = np.copy(amplitude)\n",
    "    spectrum[spectrum < rms_spect - (np.max(spectrum)-rms_spect) ] = rms_spect\n",
    "    \n",
    "    #Normalization\n",
    "    y = spectrum\n",
    "    x = velocity\n",
    "    pars = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(pars)\n",
    "    tauhi = np.array(amplitude/p(x))\n",
    "    \n",
    "    # Tau\n",
    "    tau = np.log(tauhi)*(-1)\n",
    "    rms = np.std(tau)\n",
    "    #Smoothing\n",
    "    g = Gaussian1DKernel(gkernel)\n",
    "    tau_smooth = convolve(tau, g, boundary='extend')\n",
    "    \n",
    "    if len(tau_smooth) != 1500:\n",
    "        spect_v2_1000.append(tau_smooth)\n",
    "        files_name_v2_1000.append(file_name.split('_cube')[0])\n",
    "        # if velocity[0] > -113: \n",
    "        #     spect_v2_10001.append(tau_smooth)\n",
    "        #     files_name_v2_10001.append(file_name.split('_cube')[0])\n",
    "        # else: \n",
    "        #     spect_v2_10002.append(tau_smooth)\n",
    "        #     files_name_v2_10002.append(file_name.split('_cube')[0])\n",
    "        crval3_v2_1000list.append(velocity[0])\n",
    "    else: \n",
    "        spect_v2_1500.append(tau_smooth)\n",
    "        files_name_v2_1500.append(file_name.split('_cube')[0])\n",
    "        # spect_v2_1500.append(tau_smooth)\n",
    "        # files_name_v2_1500.append(file_name.split('_cube')[0])\n",
    "        crval3_v2_1500list.append(velocity[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84b0a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(spect_v2_1000))\n",
    "print(len(spect_v2_1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "957ba3a7-b23b-43ea-9129-1a36d8cc7d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crval3_v2_1000list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e51418a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_1500 = np.zeros((len(spect_v2_1500[0]),5, 3))\n",
    "array_1000 = np.zeros((len(spect_v2_1000[0]), 4, 3))\n",
    "#array_10002 = np.zeros((len(spect_v2_10002[0]), 1, 3))\n",
    "\n",
    "\n",
    "files_v2_1500 ={}\n",
    "files_v2_1000 ={}\n",
    "#files_v2_10002 ={}\n",
    "\n",
    "# Fill the NumPy arrays with data from the lists\n",
    "for i, data in enumerate(spect_v2_1500):\n",
    "    index1 = i // 3  # Row index\n",
    "    index2 = i % 3   # Column index\n",
    "    array_1500[:, index1, index2] = data\n",
    "    \n",
    "    # Store source's name and position in the dict\n",
    "    files_v2_1500[files_name_v2_1500[i]] = {\"pos\":(index2, index1), \"vel\": crval3_v2_1500list[i]}\n",
    "\n",
    "for i, data in enumerate(spect_v2_1000):\n",
    "    index1 = i // 3  # Row index\n",
    "    index2 = i % 3   # Column index\n",
    "    array_1000[:, index1, index2] = data\n",
    "\n",
    "    # Store source's name and position in the dict\n",
    "    files_v2_1000[files_name_v2_1000[i]] = {\"pos\":(index2, index1), \"vel\": crval3_v2_1000list[i]}\n",
    "\n",
    "# for i, data in enumerate(spect_v2_10002):\n",
    "#     index1 = i // 3  # Row index\n",
    "#     index2 = i % 3   # Column index\n",
    "#     array_10002[:, index1, index2] = data\n",
    "\n",
    "#     # Store source's name and position in the dict\n",
    "#     files_v2_10002[files_name_v2_10002[i]] = {\"pos\":(index2, index1), \"index\": i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "366ae2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to interpolate NaNs\n",
    "def interpolate_nans(arr):\n",
    "    \"\"\"\n",
    "    Interpolate NaN values in a 2D array along the rows.\n",
    "    \n",
    "    Parameters:\n",
    "    arr (ndarray): Input array with NaNs to interpolate.\n",
    "    \n",
    "    Returns:\n",
    "    ndarray: Array with NaNs interpolated.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Indices where NaNs are present\n",
    "    nans = np.isnan(arr)\n",
    "    # Indices where NaNs are not present\n",
    "    x = np.arange(arr.shape[0])\n",
    "    for i in range(arr.shape[1]):\n",
    "        if np.any(nans[:, i]):\n",
    "            valid = ~nans[:, i]\n",
    "            arr[nans[:, i], i] = np.interp(x[nans[:, i]], x[valid], arr[valid, i])\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8746d4",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Interpolatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdede423",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_1500 = array_1500.reshape(1500, -1)\n",
    "# Interpolate NaNs for each component\n",
    "array_1500 = interpolate_nans(array_1500)\n",
    "\n",
    "# Reshape back to the original shape \n",
    "array_1500 = array_1500.reshape(1500, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "710b115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_1000 = array_1000.reshape(1000, -1)\n",
    "# Interpolate NaNs for each component\n",
    "array_1000 = interpolate_nans(array_1000)\n",
    "# Reshape back to the original shape \n",
    "array_1000 = array_1000.reshape(1000, 4, 3)\n",
    "\n",
    "\n",
    "# array_10002 = array_10002.reshape(1000, -1)\n",
    "# # Interpolate NaNs for each component\n",
    "# array_10002 = interpolate_nans(array_10002)\n",
    "# # Reshape back to the original shape \n",
    "# array_10002 = array_10002.reshape(1000, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58dbec",
   "metadata": {},
   "source": [
    "### Creation of FITS file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b14e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATCA cube 1 for 0.2km:  (1500, 5, 3)\n",
      "ATCA cube 2 for 0.2km:  (1000, 4, 3)\n",
      "ATCA cube for 4km:  (140, 5, 6)\n"
     ]
    }
   ],
   "source": [
    "print(\"ATCA cube 1 for 0.2km: \", array_1500.shape)\n",
    "print(\"ATCA cube 2 for 0.2km: \", array_1000.shape)\n",
    "print(\"ATCA cube for 4km: \",spect_4k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "235d5405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fits_cube(data_cube, CDELT3, CRVAL3, name, source):\n",
    "    # Create a new FITS HDU object from the NumPy \n",
    "    hdu = fits.PrimaryHDU(data_cube)\n",
    "\n",
    "    hdu.header['NAXIS'] = 3\n",
    "    hdu.header['NAXIS1'] = data_cube.shape[1]\n",
    "    hdu.header['NAXIS2'] = data_cube.shape[2]\n",
    "    hdu.header['NAXIS3'] = data_cube.shape[0]\n",
    "\n",
    "\n",
    "    hdu.header['CRPIX1'] = 1.0e+00\n",
    "    hdu.header['CDELT1'] = 1.0e+00\n",
    "    hdu.header['CRVAL1'] = 1.0e+00\n",
    "    hdu.header['CTYPE1']  = 'RA---NCP'\n",
    "    hdu.header['CUNIT1'] = 'deg'\n",
    "\n",
    "    hdu.header['CRPIX2'] = 1.0e+00\n",
    "    hdu.header['CDELT2'] = 1.0e+00\n",
    "    hdu.header['CRVAL2'] = 1.0e+00\n",
    "    hdu.header['CTYPE2']  = 'DEC--NCP'\n",
    "    hdu.header['CUNIT2'] = 'deg'\n",
    "\n",
    "\n",
    "    hdu.header['CRPIX3']  = 1.0e+00\n",
    "    hdu.header['CDELT3'] = CDELT3      #km/s\n",
    "    hdu.header['CTYPE3']  = 'VELO-LSR'\n",
    "    hdu.header['CRVAL3'] = CRVAL3      #km/s\n",
    "    hdu.header['CUNIT3'] = 'km/s' \n",
    "    \n",
    "    hdu.header['DATAMIN'] =  np.nanmin(data_cube)                        \n",
    "    hdu.header['DATAMAX'] =  np.nanmax(data_cube)   \n",
    "    \n",
    "    hdu.header['SPECSYS'] = 'LSRK' \n",
    "    hdu.header['BUNIT'] = 'JY/BEAM '                                                                                                                                                                                                                                                                                                          \n",
    "\n",
    "\n",
    "    # Save the FITS file\n",
    "    hdu.writeto(f'../Data_cubes/{source}/{name}.fits', overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2ad3ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_cube(array_1500, 0.206106359695 , -140.66372168799998, 'spectra_hydra_v2-1500_g1',source_name)\n",
    "fits_cube(array_1000, 0.206106359695 , -112.9290692, 'spectra_hydra_v2-1000_g1',source_name)\n",
    "#fits_cube(array_10002, 0.206106359695 , -160.46988205, 'spectra_v2-10002',source_name)\n",
    "fits_cube(spect_4k, 3.91596771184 , -185.686652334, 'spectra_hydra_4k_g1',source_name)\n",
    "#fits_cube(spect_4k2, 3.91596771184 ,  -230.659443842, 'spectra_4k2',source_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9b5b49-ba2a-4fd2-bd7c-4c66b8dbed5f",
   "metadata": {},
   "source": [
    "### Save Dictionaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bfaee-d8ac-405d-851b-4fa3e360e194",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_v2_1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "527de433-30d3-4cd1-b2da-424a1d98887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('../Data_cubes/ATCA/coordiantes_hydra_4k_hydra.pkl', 'wb') as f:\n",
    "    pickle.dump(files_4k, f)\n",
    "\n",
    "with open('../Data_cubes/ATCA/coordiantes_hydra_v2_1500_hydra.pkl', 'wb') as f:\n",
    "    pickle.dump(files_v2_1500, f)\n",
    "\n",
    "with open('../Data_cubes/ATCA/coordiantes_hydra_v2_1000_hydra.pkl', 'wb') as f:\n",
    "    pickle.dump(files_v2_1000, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee2e54",
   "metadata": {},
   "source": [
    "## Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1244dd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spectral_cube import SpectralCube\n",
    "\n",
    "# Path to the FITS file\n",
    "file_path1 = f'../Data_cubes/{source_name}/spectra_hydra_v2-1500.fits'\n",
    "file_path2 = f'../Data_cubes/{source_name}/spectra_hydra_v2-1000.fits'\n",
    "\n",
    "\n",
    "cube1 = SpectralCube.read(file_path1)\n",
    "cube2 = SpectralCube.read(file_path2)\n",
    "# Open the FITS file\n",
    "with fits.open(file_path1) as hdul:\n",
    "    # Print the information about the FITS file\n",
    "    hdul.info()\n",
    "    \n",
    "    # Access the primary HDU (Header/Data Unit)\n",
    "    primary_hdu1 = hdul[0]\n",
    "    \n",
    "    # Print the header of the primary HDU\n",
    "    print(primary_hdu1.header)\n",
    "    \n",
    "    # Access the data in the primary HDU\n",
    "    data1 = primary_hdu1.data\n",
    "\n",
    "# Display some basic information about the data\n",
    "print(f'Data shape: {data1.shape}')\n",
    "print(f'Data type: {data1.dtype}')\n",
    "\n",
    "# Open the FITS file\n",
    "with fits.open(file_path2) as hdul:\n",
    "    # Print the information about the FITS file\n",
    "    hdul.info()\n",
    "    \n",
    "    # Access the primary HDU (Header/Data Unit)\n",
    "    primary_hdu2 = hdul[0]\n",
    "    \n",
    "    # Print the header of the primary HDU\n",
    "    print(primary_hdu2.header)\n",
    "    \n",
    "    # Access the data in the primary HDU\n",
    "    data2 = primary_hdu2.data\n",
    "\n",
    "# Display some basic information about the data\n",
    "print(f'Data shape: {data2.shape}')\n",
    "print(f'Data type: {data2.dtype}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f3dfb8-114e-427e-8451-aba31836b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube1.spectral_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4adc254",
   "metadata": {},
   "outputs": [],
   "source": [
    "cube2.spectral_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd48fde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "ax1.plot(data1[:,1,2], linestyle='-', linewidth=1, color='g', label='Signal')\n",
    "ax1.set_ylabel(r'$\\tau$', fontsize=15)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Add legend\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(data2[:,0,0], color = 'g', label ='Smoothed')\n",
    "ax2.set_xlabel('v[Km/s]', fontsize=15)\n",
    "ax2.set_ylabel(r'$\\tau$')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4594f7-4a0c-4726-a95f-c15f1103e8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
