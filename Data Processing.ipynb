{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a583f02c-0bda-40f9-8f8a-7926f33287c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import curve_fit\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1b3c3-08ef-4016-a505-b2fe302ccfd5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09b271db-880c-4d61-9274-8efee002f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_source_names(df, mapping_dict, x_col='x_pos', y_col='y_pos',\n",
    "                       new_col='source_name', default_value='Unknown'):\n",
    "    \"\"\"\n",
    "    Assigns source names to a DataFrame based on position mapping and places the new column first.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing position columns.\n",
    "    - mapping_dict (dict): Dictionary mapping source names to position info.\n",
    "                            Expected format:\n",
    "                            {\n",
    "                                'source_name1': {'pos': (x1, y1), ...},\n",
    "                                'source_name2': {'pos': (x2, y2), ...},\n",
    "                                ...\n",
    "                            }\n",
    "    - x_col (str): Name of the x-coordinate column in df. Default is 'x_pos'.\n",
    "    - y_col (str): Name of the y-coordinate column in df. Default is 'y_pos'.\n",
    "    - new_col (str): Name of the new column to be added for source names.\n",
    "                      Default is 'source_name'.\n",
    "    - default_value (str): Value to assign if position is not found in mapping.\n",
    "                           Default is 'Unknown'.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The DataFrame with the new source name column added as the first column.\n",
    "    \"\"\"\n",
    "    # Step 1: Create a mapping from (x_pos, y_pos) to source name using dictionary comprehension\n",
    "    pos_to_source = {tuple(info['pos']): source for source, info in mapping_dict.items()\n",
    "                    if 'pos' in info and isinstance(info['pos'], (tuple, list)) and len(info['pos']) == 2}\n",
    "    \n",
    "    # Optional: Check for any sources with invalid positions and handle them\n",
    "    invalid_sources = [source for source, info in mapping_dict.items()\n",
    "                       if 'pos' not in info or not isinstance(info['pos'], (tuple, list)) or len(info['pos']) != 2]\n",
    "    if invalid_sources:\n",
    "        raise ValueError(f\"Invalid position format for sources: {invalid_sources}\")\n",
    "\n",
    "    # Step 2: Create a Series of position tuples from the DataFrame\n",
    "    positions = list(zip(df[x_col], df[y_col]))\n",
    "    \n",
    "    # Step 3: Map the positions to source names using the pos_to_source dictionary\n",
    "    source_names = pd.Series(positions).map(pos_to_source).fillna(default_value)\n",
    "    \n",
    "    # Step 4: Assign the mapped source names to the new column in the DataFrame\n",
    "    df[new_col] = source_names\n",
    "    \n",
    "    # Step 5: Reorder the columns to place 'source_name' first\n",
    "    # Only perform reordering if 'source_name' exists\n",
    "    if new_col in df.columns:\n",
    "        cols = df.columns.tolist()\n",
    "        cols.remove(new_col)\n",
    "        df = df[[new_col] + cols]\n",
    "    else:\n",
    "        raise KeyError(f\"Column '{new_col}' was not added to the DataFrame.\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1513b928-80be-40d6-a46e-674d02b224bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def modify_vlsr(df, source_dict, value):\n",
    "    \"\"\"\n",
    "    Modify the 'VLSR' column of the DataFrame based on the 'source' column and dictionary values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame with 'source' and 'VLSR' columns.\n",
    "    source_dict (dict): Dictionary where keys match df['source'] values and each has a 'vel' attribute.\n",
    "    value (float): The float value to use for the calculation.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The modified DataFrame.\n",
    "    \"\"\"\n",
    "    # Ensure the 'source' column values exist in the dictionary\n",
    "    if 'source' not in df.columns or 'VLSR' not in df.columns:\n",
    "        raise KeyError(\"'source' or 'VLSR' columns are missing in the DataFrame\")\n",
    "    \n",
    "    # Apply the modification\n",
    "    df['VLSR'] = df.apply(lambda row: row['VLSR'] + (source_dict[row['source']]['vel'] - value ), axis=1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f136219b-114f-434d-aff0-0ccf0f01c84f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_rows_by_tuples(tuples_list, df, x_col='x_pos', y_col='y_pos'):\n",
    "    \"\"\"\n",
    "    Remove rows from a DataFrame based on a list of (x, y) tuples.\n",
    "\n",
    "    Parameters:\n",
    "    tuples_list (list of tuples): A list of (x, y) pairs to match and remove.\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    x_col (str): Column name for the 'x' values. Default is 'x_pos'.\n",
    "    y_col (str): Column name for the 'y' values. Default is 'y_pos'.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A new DataFrame with the specified rows removed.\n",
    "    \"\"\"\n",
    "    for x, y in tuples_list:\n",
    "        df = df[~((df[x_col] == x) & (df[y_col] == y))]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fae9ccb-8042-4ab2-8a87-c81a4c4cba7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_fixed_columns(df_main, df_update, common_column):\n",
    "    \"\"\"\n",
    "    Update 'ra (deg)', 'dec (deg)', 'ra (hms)', and 'dec (dms)' columns in df_main with values from df_update \n",
    "    based on a common column.\n",
    "    \n",
    "    Args:\n",
    "    df_main (pd.DataFrame): The main DataFrame to be updated.\n",
    "    df_update (pd.DataFrame): The DataFrame with the updated values.\n",
    "    common_column (str): The column that both DataFrames have in common to match rows.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Updated DataFrame with values from df_update where available.\n",
    "    \"\"\"\n",
    "    columns_to_update = ['ra (deg)', 'dec (deg)', 'ra (hms)', 'dec (dms)']\n",
    "    \n",
    "    # Step 1: Perform a left merge based on the common column\n",
    "    df_merged = df_main.merge(df_update[[common_column] + columns_to_update], \n",
    "                              on=common_column, \n",
    "                              how='left', \n",
    "                              suffixes=('_main', '_update'))\n",
    "    \n",
    "    # Step 2: For each column to update, combine values from df_update where they exist\n",
    "    for col in columns_to_update:\n",
    "        df_merged[f'{col}_main'] = df_merged[f'{col}_update'].combine_first(df_merged[f'{col}_main'])\n",
    "    \n",
    "    # Step 3: Drop the extra columns from df_update and rename the main columns back\n",
    "    columns_to_drop = [f'{col}_update' for col in columns_to_update]\n",
    "    df_merged.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    # Rename columns back to their original names\n",
    "    df_merged.rename(columns={f'{col}_main': col for col in columns_to_update}, inplace=True)\n",
    "    \n",
    "    return df_merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "cb724627-ff95-458a-bb55-ca3af6fd90fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tau_calculation(velocity, amplitude):\n",
    "        # Measure variability (RMS)\n",
    "    rms_spect = np.sqrt(np.mean(amplitude**2))\n",
    "    \n",
    "    # Mask signal channels\n",
    "    spectrum = np.copy(amplitude)\n",
    "    spectrum[spectrum < rms_spect - (np.max(spectrum) - rms_spect)] = rms_spect\n",
    "    \n",
    "    # Normalization\n",
    "    y = spectrum\n",
    "    x = velocity\n",
    "    pars = np.polyfit(x, y, 1)\n",
    "    p = np.poly1d(pars)\n",
    "    tauhi = np.array(amplitude / p(x))\n",
    "    # Calculate tau (optical depth)\n",
    "    tau = np.log(tauhi) * (-1)\n",
    "    \n",
    "    # Check for NaNs in tau and interpolate missing values\n",
    "    if np.isnan(tau).any():\n",
    "        # Get indices of valid and invalid (NaN) values\n",
    "        valid_idx = np.where(~np.isnan(tau))[0]\n",
    "        invalid_idx = np.where(np.isnan(tau))[0]\n",
    "        \n",
    "        # Interpolate only over the valid points\n",
    "        interpolator = interp1d(x[valid_idx], tau[valid_idx], kind='linear', fill_value='extrapolate')\n",
    "        \n",
    "        # Apply interpolation to the NaN values\n",
    "        tau[invalid_idx] = interpolator(x[invalid_idx])\n",
    "\n",
    "    # Return the tau values with NaNs handled\n",
    "    return tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "666a39f2-c251-4458-94c6-9780141cb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Gaussian function\n",
    "def gaussian(x, amp, mean, vel_disp):\n",
    "    return amp * np.exp(-0.5 * ((x - mean) / vel_disp)**2)\n",
    "\n",
    "# Function to apply the Gaussian fitting and plot results\n",
    "def restore_amplitudes_df(df, directory, category, plotting = True):\n",
    "    df['new_amp'] = np.nan\n",
    "    df['new_VLSR'] = np.nan\n",
    "    df['new_vel_disp'] = np.nan\n",
    "    # Group by 'source'\n",
    "    grouped_df = df.groupby('source')\n",
    "    \n",
    "    # Loop over each group (each 'source')\n",
    "    for source, group in grouped_df:\n",
    "        # Load the ATCA data for this source\n",
    "        if category == \"atca_4k\":\n",
    "            file_path = os.path.join(directory, f'{source}_cube_4k_atca_4k_spectrum.txt')\n",
    "        elif category == \"atca_v2\":\n",
    "            file_path = os.path.join(directory, f'{source}_cube_v2_atca_spectrum.txt')\n",
    "        elif category == \"askap\":\n",
    "            file_path = os.path.join(directory, f'{source}_askap_spectrum.txt')\n",
    "        \n",
    "        data = np.loadtxt(file_path, skiprows=1)\n",
    "        velocity = data[:, 1]  # velocity column\n",
    "        amplitude = data[:, 2]  # amplitude column\n",
    "        \n",
    "        # Apply tau calculation (you should provide this function)\n",
    "        amplitude = tau_calculation(velocity, amplitude)\n",
    "        \n",
    "        # Initialize plot\n",
    "        if plotting == True:\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            plt.plot(velocity, amplitude, label='Signal', color='k', lw=0.4)\n",
    "            total_fit = np.zeros_like(velocity)\n",
    "            ncomps = 0\n",
    "        # Loop through each row in the group (each component)\n",
    "        for idx, row in group.iterrows():\n",
    "            amp, mean, vel_disp = row['amp'], row['VLSR'], row['vel_disp']\n",
    "            # Perform Gaussian fitting\n",
    "            popt, pcov = curve_fit(gaussian, velocity, amplitude, p0=[amp, mean, vel_disp])\n",
    "            # Store the fitted parameters in the DataFrame\n",
    "            df.at[idx, 'new_amp'] = popt[0]\n",
    "            df.at[idx, 'new_VLSR'] = popt[1]\n",
    "            df.at[idx, 'new_vel_disp'] = popt[2]\n",
    "            # Plot the fitted Gaussian curve\n",
    "            if plotting == True:\n",
    "                fitted_curve = gaussian(velocity, *popt)\n",
    "                total_fit += fitted_curve\n",
    "                plt.plot(velocity, fitted_curve, color='red', lw = 0.4)\n",
    "                ncomps += 1\n",
    "        if plotting == True:\n",
    "            # Add labels and legend\n",
    "            plt.plot(velocity, total_fit, color='r', lw=1.8)\n",
    "            plt.axhline(0, color='k', linestyle='-', linewidth=0.8)\n",
    "            plt.title(f'Gaussian Fit for Source: {source}' r'$\\quad N_{comp}=$' f'{ncomps}')\n",
    "            plt.xlabel('VELO-LSR [km / s]')\n",
    "            plt.ylabel('Intensity')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a09c45-16ab-4b84-8f29-eb1f6d032ca7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adaa60b6-b473-43f0-8716-1e826683eced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ASKAP\n",
    "file_path_hydra_askap = \"RESULTS/decomposition_hydra/gpy_decomposed/spectra_hydra_raw_g+_fit_fin_finalized.dat\"\n",
    "file_path_norma_askap = \"RESULTS/decomposition_norma/gpy_decomposed/spectra_norma_raw_g+_fit_fin_finalized.dat\"\n",
    "\n",
    "#ATCA\n",
    "\n",
    "file_path_hydra4k_atca = \"RESULTS/decomposition_hydra4k/gpy_decomposed/spectra_hydra_4k_g1_g+_fit_fin_finalized.dat\"\n",
    "file_path_hydra1000_atca = \"RESULTS/decomposition_hydraV21000/gpy_decomposed/spectra_hydra_v2-1000_g1_g+_fit_fin_finalized.dat\"\n",
    "file_path_hydra1500_atca = \"RESULTS/decomposition_hydraV21500/gpy_decomposed/spectra_hydra_v2-1500_g1_g+_fit_fin_finalized.dat\"\n",
    "\n",
    "file_path_norma4k_atca= \"RESULTS/decomposition_norma4k/gpy_decomposed/spectra_norma_4k_raw_g+_fit_fin_finalized.dat\"\n",
    "file_path_norma1000_atca = \"RESULTS/decomposition_normaV21000/gpy_decomposed/spectra_norma_v2-1000_g1_g+_fit_fin_finalized.dat\"\n",
    "file_path_norma1500_atca = \"RESULTS/decomposition_normaV21500/gpy_decomposed/spectra_norma_v2-1500_g1_g+_fit_fin_finalized.dat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad73b6e7-17eb-465d-9501-407575da1cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the file into a DataFrame\n",
    "df_hydra_askap = pd.read_csv(file_path_hydra_askap, delim_whitespace=True)\n",
    "df_norma_askap = pd.read_csv(file_path_norma_askap, delim_whitespace=True)\n",
    "\n",
    "\n",
    "df_hydra4k_atca = pd.read_csv(file_path_hydra4k_atca, delim_whitespace=True)\n",
    "df_h1000_atca = pd.read_csv(file_path_hydra1000_atca, delim_whitespace=True)\n",
    "df_h1500_atca = pd.read_csv(file_path_hydra1500_atca, delim_whitespace=True)\n",
    "\n",
    "df_norma4k_atca = pd.read_csv(file_path_norma4k_atca, delim_whitespace=True)\n",
    "df_n1000_atca = pd.read_csv(file_path_norma1000_atca, delim_whitespace=True)\n",
    "df_n1500_atca = pd.read_csv(file_path_norma1500_atca, delim_whitespace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ce7132-7fe4-4fe9-8cb4-64cd77095b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Data_cubes/ATCA/coordiantes_norma_4k_hydra.pkl', 'rb') as f:\n",
    "    dict_4k_n = pickle.load(f)\n",
    "with open('Data_cubes/ATCA/coordiantes_hydra_4k_hydra.pkl', 'rb') as f:\n",
    "    dict_4k_h = pickle.load(f)\n",
    "\n",
    "with open('Data_cubes/ATCA/coordiantes_norma_v2_1000_hydra.pkl', 'rb') as f:\n",
    "    dict_v21000_n = pickle.load(f)\n",
    "with open('Data_cubes/ATCA/coordiantes_hydra_v2_1000_hydra.pkl', 'rb') as f:\n",
    "    dict_v21000_h = pickle.load(f)\n",
    "\n",
    "with open('Data_cubes/ATCA/coordiantes_norma_v2_1500_hydra.pkl', 'rb') as f:\n",
    "    dict_v21500_n = pickle.load(f)\n",
    "with open('Data_cubes/ATCA/coordiantes_hydra_v2_1500_hydra.pkl', 'rb') as f:\n",
    "    dict_v21500_h = pickle.load(f)\n",
    "\n",
    "\n",
    "#ASKAP\n",
    "with open('Data_cubes/ASKAP/coordinates_norma.pkl', 'rb') as f:\n",
    "    dict_n_askap = pickle.load(f)\n",
    "with open('Data_cubes/ASKAP/coordinates_hydra.pkl', 'rb') as f:\n",
    "    dict_h_askap = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07460484-bdcf-4f4d-83f1-4cdb92380526",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Name assignation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c60c0a8-7d08-4070-b7d3-a9b3e5cdf6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign source names using the function\n",
    "df_h1000_atca = assign_source_names(df_h1000_atca, dict_v21000_h,\n",
    "                                x_col='x_pos', y_col='y_pos',\n",
    "                                new_col='source', default_value='Unknown')\n",
    "# Assign source names using the function\n",
    "df_h1500_atca = assign_source_names(df_h1500_atca, dict_v21500_h,\n",
    "                                x_col='x_pos', y_col='y_pos',\n",
    "                                new_col='source', default_value='Unknown')\n",
    "\n",
    "# Assign source names using the function\n",
    "df_hydra4k_atca = assign_source_names(df_hydra4k_atca, dict_4k_h,\n",
    "                                x_col='x_pos', y_col='y_pos',\n",
    "                                new_col='source', default_value='Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7edf1f0-6db7-4529-bf0b-2286c0719b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign source names using the function\n",
    "df_norma4k_atca = assign_source_names(df_norma4k_atca, dict_4k_n,\n",
    "                                x_col='x_pos', y_col='y_pos',\n",
    "                                new_col='source', default_value='Unknown')\n",
    "df_n1000_atca = assign_source_names(df_n1000_atca, dict_v21000_n,\n",
    "                                x_col='x_pos', y_col='y_pos',\n",
    "                                new_col='source', default_value='Unknown')\n",
    "df_n1500_atca = assign_source_names(df_n1500_atca, dict_v21500_n,\n",
    "                                x_col='x_pos', y_col='y_pos',\n",
    "                                new_col='source', default_value='Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe8594bc-01d0-43e4-af91-f36f6a98af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hydra_askap = assign_source_names(df_hydra_askap, dict_h_askap,\n",
    "                                x_col='x_pos', y_col='y_pos',\n",
    "                                new_col='source', default_value='Unknown')\n",
    "df_norma_askap = assign_source_names(df_norma_askap, dict_n_askap,\n",
    "                                x_col='x_pos', y_col='y_pos',\n",
    "                                new_col='source', default_value='Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a5ef22-f544-437e-9fda-a2af8841bebb",
   "metadata": {},
   "source": [
    "## VLSR and Amplitude correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9696be65-0ba5-4b91-876f-3ea2d8e85f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h1000_atca = modify_vlsr(df_h1000_atca, dict_v21000_h, -112.9290692)\n",
    "df_h1500_atca = modify_vlsr(df_h1500_atca, dict_v21500_h, -140.6637217)\n",
    "df_hydra4k_atca = modify_vlsr(df_hydra4k_atca, dict_4k_h, -185.6866523)\n",
    "\n",
    "df_norma4k_atca = modify_vlsr(df_norma4k_atca, dict_4k_n, -189.3374977)\n",
    "df_n1000_atca = modify_vlsr(df_n1000_atca, dict_v21000_n, -119.4194126)\n",
    "df_n1500_atca = modify_vlsr(df_n1500_atca, dict_v21500_n, -146.5113794)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6dd52a8-53c6-4812-9af0-d0fc4f645bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_norma_v2_1000 = [(2,1)]\n",
    "remove_norma_v2_1500 = [(0,5),(2,5)]\n",
    "remove_hydra_v2_1000 = [(2,0),(1,1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59ad7cf2-b66e-478f-8b93-eb0eedd17435",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n1000_atca = remove_rows_by_tuples(remove_norma_v2_1000, df_n1000_atca)\n",
    "df_n1500_atca = remove_rows_by_tuples(remove_norma_v2_1500, df_n1500_atca)\n",
    "df_h1000_atca = remove_rows_by_tuples(remove_norma_v2_1000, df_h1000_atca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8509cf1-d05d-4fa7-9e82-d79a6138535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_n1000_atca = restore_amplitudes_df(df_n1000_atca, amp_column='amp', sigma=1)\n",
    "df_n1500_atca = restore_amplitudes_df(df_n1500_atca, amp_column='amp', sigma=1)\n",
    "df_h1000_atca = restore_amplitudes_df(df_h1000_atca, amp_column='amp', sigma=1)\n",
    "df_h1500_atca = restore_amplitudes_df(df_h1500_atca, amp_column='amp', sigma=1)\n",
    "df_hydra4k_atca = restore_amplitudes_df(df_hydra4k_atca, amp_column='amp', sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3c90107d-965e-44e3-b99a-5d40067c0ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hydraV2_atca = pd.concat([df_h1000_atca, df_h1500_atca ], axis = 0, ignore_index = True)\n",
    "\n",
    "df_normaV2_atca = pd.concat([df_n1000_atca, df_n1500_atca], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1e1e0021-b5e4-47f0-be48-ff8158c931aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93/2689269635.py:16: RuntimeWarning: invalid value encountered in log\n",
      "  tau = np.log(tauhi) * (-1)\n"
     ]
    }
   ],
   "source": [
    "new_df = restore_amplitudes_df(df_hydraV2_atca, \n",
    "             'ATCA_HI_spectra/Hydra/', \n",
    "             category = \"atca_v2\",\n",
    "             plotting = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a05a2c1-4b6a-443d-80d8-3589465d8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASKAP - Hydra - number of spectral signals:  43\n",
      "ASKAP - Normal - number of spectral signals:  325\n",
      "ATCA - Hydra - number of spectral signals v=4:  16\n",
      "ATCA - Hydra - number of spectral signals v=0.2:  26\n",
      "ATCA - Norma - number of spectral signals v=4:  59\n",
      "ATCA - Norma - number of spectral signals v=0.2:  41\n"
     ]
    }
   ],
   "source": [
    "print('ASKAP - Hydra - number of spectral signals: ',len(df_hydra_askap))\n",
    "print('ASKAP - Normal - number of spectral signals: ',len(df_norma_askap))\n",
    "print('ATCA - Hydra - number of spectral signals v=4: ',len(df_hydra4k_atca))\n",
    "print('ATCA - Hydra - number of spectral signals v=0.2: ',len(df_hydraV2_atca))\n",
    "print('ATCA - Norma - number of spectral signals v=4: ',len(df_norma4k_atca))\n",
    "print('ATCA - Norma - number of spectral signals v=0.2: ',len(df_normaV2_atca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aad24051-ffc0-4a04-a875-9dc64dd3d897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sources decomposed from ASKAP-Norma:  259\n",
      "Sources decomposed from ASKAP-Hydra:  42\n",
      "Sources decomposed from ATCA-Norma 4km/s:  40\n",
      "Sources decomposed from ATCA-Norma 0.2km/s:  18\n",
      "Sources decomposed from ATCA-Hydra 4km/s:  14\n",
      "Sources decomposed from ATCA-Hydra 0.2km/s:  17\n"
     ]
    }
   ],
   "source": [
    "print('Sources decomposed from ASKAP-Norma: ',df_norma_askap['source'].nunique())\n",
    "print('Sources decomposed from ASKAP-Hydra: ',df_hydra_askap['source'].nunique())\n",
    "print('Sources decomposed from ATCA-Norma 4km/s: ',df_norma4k_atca['source'].nunique())\n",
    "print('Sources decomposed from ATCA-Norma 0.2km/s: ',df_normaV2_atca['source'].nunique())\n",
    "print('Sources decomposed from ATCA-Hydra 4km/s: ',df_hydra4k_atca['source'].nunique())\n",
    "print('Sources decomposed from ATCA-Hydra 0.2km/s: ',df_hydraV2_atca['source'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321cf6a1-8fe9-4ce8-a83d-0a46b77d9b41",
   "metadata": {},
   "source": [
    "## Coordinates assignation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9664a770-9bf7-42b8-9f14-ac4f2e459234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "df_hydraV2_atca.rename(columns={\n",
    "    'x_pos': 'ra (deg)',\n",
    "    'y_pos': 'dec (deg)',\n",
    "    'RA': 'ra (hms)',\n",
    "    'DEC': 'dec (dms)'\n",
    "}, inplace=True)\n",
    "\n",
    "df_hydra4k_atca.rename(columns={\n",
    "    'x_pos': 'ra (deg)',\n",
    "    'y_pos': 'dec (deg)',\n",
    "    'RA': 'ra (hms)',\n",
    "    'DEC': 'dec (dms)'\n",
    "}, inplace=True)\n",
    "\n",
    "# Renaming the columns\n",
    "df_normaV2_atca.rename(columns={\n",
    "    'x_pos': 'ra (deg)',\n",
    "    'y_pos': 'dec (deg)',\n",
    "    'RA': 'ra (hms)',\n",
    "    'DEC': 'dec (dms)'\n",
    "}, inplace=True)\n",
    "\n",
    "df_norma4k_atca.rename(columns={\n",
    "    'x_pos': 'ra (deg)',\n",
    "    'y_pos': 'dec (deg)',\n",
    "    'RA': 'ra (hms)',\n",
    "    'DEC': 'dec (dms)'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a0e6dc5-54cf-4e51-9d64-8dd56a900095",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_h = pd.read_csv(f'ATCA_HI_spectra/hydra_coords.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d791e69-0e3b-43b2-84bd-baa9d38b040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_n = pd.read_csv(f'ATCA_HI_spectra/norma_coords.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f43a0af-3ef4-4c7d-8731-fd65c5880205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "df_normaV2_atca = update_fixed_columns(\n",
    "    df_main=df_normaV2_atca,   # The main DataFrame\n",
    "    df_update=sources_n,       # The DataFrame with updated values\n",
    "    common_column='source'     # The common column to match rows\n",
    ")\n",
    "df_norma4k_atca = update_fixed_columns(\n",
    "    df_main=df_norma4k_atca,   # The main DataFrame\n",
    "    df_update=sources_n,       # The DataFrame with updated values\n",
    "    common_column='source'     # The common column to match rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4dfbafa-ec68-4004-9582-b83173266bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hydraV2_atca = update_fixed_columns(\n",
    "    df_main=df_hydraV2_atca,   # The main DataFrame\n",
    "    df_update=sources_h,       # The DataFrame with updated values\n",
    "    common_column='source'     # The common column to match rows\n",
    ")\n",
    "\n",
    "df_hydra4k_atca = update_fixed_columns(\n",
    "    df_main=df_hydra4k_atca,   # The main DataFrame\n",
    "    df_update=sources_h,       # The DataFrame with updated values\n",
    "    common_column='source'     # The common column to match rows\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0ac18ee-63cf-4189-97b2-99e68c27c6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normaV2_atca['source'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab00b4b6-06f5-43f0-81a2-3dbd28fb40ca",
   "metadata": {},
   "source": [
    "## Coordinates ASKAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7775d748-8b20-4801-8cfd-f6ae3d1a366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hydra_askap = modify_vlsr(df_hydra_askap, dict_h_askap, -446.171913620222)\n",
    "df_norma_askap = modify_vlsr(df_norma_askap, dict_n_askap, -437.311662453059)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cc65228-2db4-4c59-87ff-a4e9cacb10d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the columns\n",
    "df_hydra_askap.rename(columns={\n",
    "    'x_pos': 'ra (deg)',\n",
    "    'y_pos': 'dec (deg)',\n",
    "    'RA': 'ra (hms)',\n",
    "    'DEC': 'dec (dms)'\n",
    "}, inplace=True)\n",
    "\n",
    "df_norma_askap.rename(columns={\n",
    "    'x_pos': 'ra (deg)',\n",
    "    'y_pos': 'dec (deg)',\n",
    "    'RA': 'ra (hms)',\n",
    "    'DEC': 'dec (dms)'\n",
    "}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "064fe1c4-f0ab-444c-942e-2131071ac863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the 'ASKAP_spectra/Hydra/' directory path\n",
    "directory_path_hydra = 'ASKAP_spectra/Hydra/'\n",
    "directory_path_norma = 'ASKAP_spectra/Norma/'\n",
    "\n",
    "# List all filenames in the directory\n",
    "filenames_hydra = [f for f in os.listdir(directory_path_hydra) if f.endswith('_askap_spectrum.txt')]\n",
    "filenames_norma = [f for f in os.listdir(directory_path_norma) if f.endswith('_askap_spectrum.txt')]\n",
    "\n",
    "# Function to convert RA in HMS to degrees\n",
    "def ra_hms_to_deg(h, m, s):\n",
    "    return 15 * (int(h) + int(m)/60 + float(s)/3600)\n",
    "\n",
    "# Function to convert Dec in DMS to degrees\n",
    "def dec_dms_to_deg(sign, d, m, s):\n",
    "    dec_deg = int(d) + int(m)/60 + float(s)/3600\n",
    "    return -dec_deg if sign == '-' else dec_deg\n",
    "\n",
    "# Function to extract the source, RA, and Dec from the filename\n",
    "def extract_coordinates(filename):\n",
    "    # Extract the RA and Dec from the filename\n",
    "    name = filename.split('_')[0]\n",
    "    \n",
    "    # Source name (optional)\n",
    "    source = name[:15]  # first 6 characters as example source\n",
    "    \n",
    "    # Right Ascension (RA)\n",
    "    ra_hours = name[:2]\n",
    "    ra_minutes = name[2:4]\n",
    "    ra_seconds = name[4:8]\n",
    "    ra_hms = f\"{ra_hours}:{ra_minutes}:{ra_seconds}\"\n",
    "    ra_deg = ra_hms_to_deg(ra_hours, ra_minutes, ra_seconds)\n",
    "    \n",
    "    # Declination (Dec)\n",
    "    sign = name[8]\n",
    "    dec_degrees = name[9:11]\n",
    "    dec_arcminutes = name[11:13]\n",
    "    dec_arcseconds = name[13:15]\n",
    "    dec_dms = f\"{sign}{dec_degrees}:{dec_arcminutes}:{dec_arcseconds}\"\n",
    "    dec_deg = dec_dms_to_deg(sign, dec_degrees, dec_arcminutes, dec_arcseconds)\n",
    "    \n",
    "    return source, ra_deg, dec_deg, ra_hms, dec_dms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9e289a9-7ab2-4181-a058-62b0163bbab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples with source, RA in deg, Dec in deg, RA in HMS, Dec in DMS\n",
    "coordinates_hydra = [extract_coordinates(filename) for filename in filenames_hydra]\n",
    "coordinates_norma = [extract_coordinates(filename) for filename in filenames_norma]\n",
    "\n",
    "\n",
    "# Create the DataFrame with the desired columns\n",
    "df_hydra_askap_names = pd.DataFrame(coordinates_hydra, columns=['source', 'ra (deg)', 'dec (deg)', 'ra (hms)', 'dec (dms)'])\n",
    "df_norma_askap_names = pd.DataFrame(coordinates_norma, columns=['source', 'ra (deg)', 'dec (deg)', 'ra (hms)', 'dec (dms)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d403f8fb-9e94-4f03-9c91-f9aee2169509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hydra_askap = update_fixed_columns(\n",
    "    df_main=df_hydra_askap,   # The main DataFrame\n",
    "    df_update=df_hydra_askap_names,       # The DataFrame with updated values\n",
    "    common_column='source'     # The common column to match rows\n",
    ")\n",
    "\n",
    "df_norma_askap = update_fixed_columns(\n",
    "    df_main=df_norma_askap,   # The main DataFrame\n",
    "    df_update=df_norma_askap_names,       # The DataFrame with updated values\n",
    "    common_column='source'     # The common column to match rows\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb5838-e625-4576-bc12-1dd5a3dee414",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aedda04c-7e7a-4043-82fa-15a29a3cbb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrames as pickle files with appropriate names\n",
    "df_norma_askap.to_pickle('RESULTS/norma_askap.pkl')\n",
    "df_hydra_askap.to_pickle('RESULTS/hydra_askap.pkl')\n",
    "df_normaV2_atca.to_pickle('RESULTS/norma_atca_0.2kms.pkl')\n",
    "df_norma4k_atca.to_pickle('RESULTS/norma_atca_4kms.pkl')\n",
    "df_hydraV2_atca.to_pickle('RESULTS/hydra_atca_0.2kms.pkl')\n",
    "df_hydra4k_atca.to_pickle('RESULTS/hydra_atca_4kms.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8440ea6-1421-4e73-adf6-c9ba0443cc20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
